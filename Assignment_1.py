# -*- coding: utf-8 -*-
"""Assignment_1_Instructions_and_Template_downloaded.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-uNgHIapS0aM3B_mFMDujARWpQA38zW2

#**EE769 Introduction to Machine Learning**

#Assignment 1: Gradient Descent, Linear Regression, and Regularization


**Template and Instructions**

#**Part 1 begins ...**
**Instructions to be strictly followed:**

We cannot ensure correct grading if you change anything else, and you may be penalised for not following these instructions.

## Import Statements
"""

import numpy as np                                                              # Importing Numpy, Pandas and Matplotlib libraries
import pandas as pd
from matplotlib import pyplot as plt

"""## Normalize function 


"""

def Normalize(X):                                                               # Output is normalized data matrix of the same dimension with mean '0' and standard deviation '1'
    '''
    Normalize all columns of X using mean and standard deviation
    '''
    # YOUR CODE HERE
    # YOUR CODE HERE
    X_mean =np.mean(X, axis=0)                                                  # Computing the mean of numpy array. Axis = 0 means coloumn wise mean will be computed. 
    X_std_deviation = np.std(X,axis =0)                                         # Computing the standard deviation of numpy array. Axis = 0 means coloumn wise std devation will be computed. 
    X = np.divide(np.subtract(X,X_mean),X_std_deviation)                        # Normalized variable value = (variable value - mean)/standard deviation
    return X
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - 1 dimensional array'''
#X=np.array([[1,2,3],[3,4,5],[7,8,9]])
X1=np.array([1,2,3])
np.testing.assert_array_almost_equal(Normalize(X1),np.array([-1.224,  0.      ,  1.224]),decimal=3)
''' case 2 - 2 dimensional array'''
X2=np.array([[4,7,6],[3,8,9],[5,11,10]])
np.testing.assert_array_almost_equal(Normalize(X2),np.array([[ 0.  , -0.980581, -1.372813],[-1.224745, -0.392232,  0.392232],[ 1.224745,  1.372813,  0.980581]]))
''' case 3 - 1 dimensional array with float'''
X3=np.array([5.5,6.7,3.2,6.7])
np.testing.assert_array_almost_equal(Normalize(X3),np.array([-0.017,  0.822, -1.627,  0.822]),decimal=3)

"""## Prediction Function

Given X and w, compute the predicted output. Do not forget to add 1's in X
"""

def Prediction (X, w):                                                          # Predicts the output. 'X' is input matrix and 'w' is weight vector
    '''
    Compute Prediction given an input datamatrix X and weight vecor w. Output y = [X 1]w where 1 is a vector of all 1s 
    '''
    # YOUR CODE HERE
    shape_=X.shape                                                              # To find out the shape of input matrix
    coloumn_matrix_ones = np.ones((shape_[0],1))                                # Creating a coloumn matrix of '1's as per size of input matrix. The last coloumn of '1' is for the bias term in the weight vector
    X=np.append(X, coloumn_matrix_ones, axis=1)                                 # Appending the coloumn matrix of '1's as last coloumn of input matrix
    return np.dot(X,w)                                                          # Computing the predicted value as matrix product of X and W
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - Known input output matrix and weights 1'''
X1 = np.array([[3,2],[1,1]])
w1 = np.array([2,1,1]) 
np.testing.assert_array_equal(Prediction(X1,w1),np.array([9,4]))

"""## Loss Functions

Code the four  loss functions:

1. MSE loss is only for the error
2. MAE loss is only for the error
3. L2 loss is for MSE and L2 regularization, and can call MSE loss
4. L1 loss is for MSE and L1 regularization, and can call MSE loss
"""

def MSE_Loss (X, t, w, lamda =0):                                               # Function to compute MSE loss
    '''
    lamda=0 is a default argument to prevent errors if you pass lamda to a function that doesn't need it by mistake. 
    This allows us to call all loss functions with the same input format.
    
    You are encouraged read about default arguments by yourself online if you're not familiar.
    '''
    # YOUR CODE HERE
    shape_ =X.shape                                                                # To find out the shape of input matrix
    predicted_value= Prediction(X, w)                                              # Computing the predicted value as matrix product of X and W
    return np.divide(np.sum(np.square(np.subtract(predicted_value,t))),shape_[0])  # Computing mean square error . MSE is mean of sum of squares of difference between predicted and test input) 
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(MSE_Loss(X,t,w),0.53,decimal=3)

def MAE_Loss (X, t, w, lamda = 0):                                                  # Function to computer MAE loss
    # YOUR CODE HERE
    shape_=X.shape                                                                  # To find out the shape of input matrix
    predictced_value= Prediction(X, w)                                              # Computing the predicted value as matrix product of X and W
    return np.divide(np.sum(np.abs(np.subtract(predictced_value,t))),shape_[0])     # Computing mean absolute error . MAE is mean of sum of absolute value of difference between predicted and test input) 
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(MAE_Loss(X,t,w),0.700,decimal=3)

def L2_Loss (X, t, w, lamda):                                                       # Function to compute L2 loss
    ''' Need to specify what inputs are'''
    # YOUR CODE HERE
    shape_=X.shape                                                                  # To find out the shape of input matrix
    cost =MSE_Loss(X,t,w)                                                           # Computing mean square error . MSE is mean of sum of squares of difference between predicted and test input)
    regularization_ = np.sqrt(np.sum(np.square(w)[0:len(w)-1]))*lamda               #Regularization = lamda*L2 Norm (except for the bias term)
    return (cost+regularization_)
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(L2_Loss(X,t,w,0.5),1.675,decimal=3)

def L1_Loss (X, t, w, lamda):                                                       # Function to compute L1 loss 
    # YOUR CODE HERE
    shape_=X.shape                                                                  # To find out the shape of input matrix
    predicted_value= Prediction(X, w)                                               # Computing the predicted value as matrix product of X and W
    cost =np.divide(np.sum(np.square(np.subtract(predicted_value,t))),(shape_[0]))  # Computing mean square error . MSE is mean of sum of squares of difference between predicted and test input)
    regularization_ = np.sum(np.abs(w)[0:len(w)-1])*lamda                           #Regularization = lamda*L1 Norm (except for the bias term)
    return (cost+regularization_)
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(L1_Loss(X,t,w,0.5),2.280,decimal=3)

def NRMSE_Metric (X, t, w, lamda=0):                                                # Function to compute NMRSE metric
    # YOUR CODE HERE
    shape_=X.shape                                                                  # To find out the shape of input matrix
    predicted_value= Prediction(X, w)                                               # Computing the predicted value as matrix product of X and W
    rmse =np.sqrt(np.divide(np.sum(np.square(np.subtract(predicted_value,t))),(shape_[0])))           # Computing mean square error . MSE is mean of sum of squares of difference between predicted and test input)
    std_deviation =np.std(t)
    return (rmse/std_deviation)                                                     #NRMSE =RMSE/Standard deviation
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' Test case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(NRMSE_Metric(X,t,w,0.5),0.970,decimal=3)

"""## Gradient function
Each Loss function will have its own gradient function:

1. MSE gradient is only for the error
2. MAE gradient is only for the error
3. L2 gradient is for MSE and L2 regularization, and can call MSE gradient
4. L1 gradient is for MSE and L1 regularization, and can call MSE gradient
"""

def MSE_Gradient (X, t, w, lamda=0):                                                # Function to compute MSE Gradient
    # YOUR CODE HERE
    shape_=X.shape                                                                  # To find out the shape of input matrix
    predicted_value= Prediction(X, w)                                               # Computing the predicted value as matrix product of X and W
    diff =np.subtract(t,predicted_value)                                            #Intermediate term obtained when differentiating MSE loss function wrt Wi.
    coloumn_matrix_ones = np.ones((shape_[0],1))                                    # Creating a coloumn matrix of '1's as per size of input matrix
    X=np.append(X, coloumn_matrix_ones, axis=1)                                     # Appending coloumn of '1's to X vector to take dot product

    gradient =(np.dot(diff.T,X)*(-2))/shape_[0] #TODO                               #gradient equation is derived by differentiating MSE loss function wrt Wi.
    return gradient
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(MSE_Gradient(X,t,w),np.array([2.55, 2.94, 2.9 , 0.4 ]),decimal=3)

def MAE_Gradient (X, t, w, lamda=0):                                                # Function to compute MAE gradient
    # YOUR CODE HERE
    shape_=X.shape                                                                  # To find out the shape of input matrix
    predicted_value= Prediction(X, w)                                               # Computing the predicted value as matrix product of X and W
    diff =np.subtract(predicted_value,t)                                            #Intermediate term obtained when differentiating MSE loss function wrt Wi.
    diff_sign = np.where(diff>=0,1,-1)                                              #Intermediate term obtained when differentiating MSE loss function wrt Wi.
    coloumn_matrix_ones = np.ones((shape_[0],1))                                    # Creating a coloumn matrix of '1's as per size of input matrix
    X=np.append(X, coloumn_matrix_ones, axis=1)                                     # Appending the coloumn matrix of '1's as last coloumn of input matrix
    gradient =(np.dot(diff_sign,X))/(shape_[0])                                     #gradient equation is derived by differentiating MAE loss function wrt Wi.
    return gradient
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(MAE_Gradient(X,t,w),np.array([0.75,  0.3 ,  0.5 , 0.]),decimal=3)

def L2_Gradient (X, t, w, lamda):                                                                 # Function to compute L2 gradient
    # YOUR CODE HERE
    gradient_mse =MSE_Gradient (X, t, w, lamda=0)                                                 # Computer MSE gradient part of L2 gradient
    gradient_l2 =np.divide((lamda*w),np.sqrt(np.sum(np.square(w)[0:len(w)-1])))[0:len(w)-1]       #Derivative of (Lambda*L2 Norm of W) wrt Wi
    gradient_l2 =np.append(gradient_l2,0)                                                         #Derivative of (Lambda*L2 Norm of W) wrt W0 is '0' as W0 is not used for computing regularization factor. Therefore '0' is appended.    
    return gradient_mse+gradient_l2
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(L2_Gradient(X,t,w,0.5),np.array([2.986, 2.721, 3.009 , 0.4 ]),decimal=3)

def L1_Gradient (X, t, w, lamda):                                                               # Function to compute L1 gradient
    # YOUR CODE HERE
    gradient_mse =MSE_Gradient (X, t, w, lamda=0)                                               # Computer MSE gradient part of L2 gradient
    diff_sign = np.where(w>=0,1,-1)                                                             # Derivative of (Lambda*L1 Norm of W) wrt Wi
    gradient_l1 =(diff_sign*0.5)[0:len(w)-1]                                                    #Bias term is excluded from L1 gradient computation.
    gradient_l1 =np.append(gradient_l1,0)                                                       #Derivative of (Lambda*L2 Norm of W) wrt W0 is '0' as W0 is not used for computing regularization factor. Therefore '0' is appended.
    return gradient_mse+gradient_l1
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(L1_Gradient(X,t,w,0.5),np.array([3.05, 2.44, 3.4 , 0.4 ]),decimal=3)

"""## Gradient Descent Function

"""

def Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, lossfunc, gradfunc): # Gradient Descent function
    # YOUR CODE HERE
    iterations =0                                                                               # Variable to count the no of iterations.
    loss_train_prev =-10                                                                        # loss value will be always +ve. A -ve no is is assigned to avoid a collision (chance that first loss value matches initial value given) during first loop.
    exit_cnt =0                                                                                 # Variable for exit condition. If there  is no improvement in valiation loss for 5 consecutive cycles, gradient descent loop will be stopped.    
    while (iterations<(max_iter)):                                                              # While loop till maximum no of iterations 
        gradient_=gradfunc(X,t,w,lamda)                                                         #Computing gradient 
        w=w-(lr*gradient_.T)                                                                    #Computer the new weight vector
        loss_train =lossfunc(X,t,w,lamda)                                                       #Train data loss  
        loss_val = lossfunc(X_val,t_val,w,lamda)                                                #Validation data loss
        iterations+=1                                                                           #Incrementing the iterator
        if(loss_train_prev <= loss_train):                                                      #If there is no improvement in loss function for 5 consecutive cycles, gradient descent will be stopped.
          exit_cnt +=1
        else:
          exit_cnt=0
        if(exit_cnt==5):
          break
        loss_train_prev =loss_train
    validation_NRMSE=NRMSE_Metric (X, t, w, lamda=0)                                            #Computing NRMSE metric.
    #print(loss_train,loss_val)
    return w, loss_train, loss_val, validation_NRMSE #You should return variables structured like this.
    raise NotImplementedError()
    #return w_final, train_loss_final, validation_loss_final, validation_NRMSE #You should return variables structured like this.

'''
TEST CASES, DO NOT CHANGE
'''
X=np.array([[23,24],[1,2]])
t=np.array([4,5])
X_val=np.array([[3,4],[5,6]])
t_val=np.array([3,4])
w=np.array([3,2,1])
results =Gradient_Descent (X, X_val, t, t_val, w, 0.1, 100, 1e-10, 1e-5, L2_Loss,L2_Gradient) 
np.testing.assert_allclose([results[1]],[697.919],rtol =0.05)
np.testing.assert_allclose([results[2]],[20],atol=5) # we expect around 17.5  but some students got 24 which we will also accept
#Instructor Values of results[1] and results [2] are 697.919 and 17.512 respectively

"""## Pseudo Inverse Method

You have to implement a slightly more advanced version, with L2 penalty:

w = (X' X + lambda I)^(-1) X' t.

See, for example: Section 2 of https://web.mit.edu/zoya/www/linearRegression.pdf

Here, the column of 1's in assumed to be included in X
"""

def Pseudo_Inverse (X, t, lamda):                                               # Function to computer Pseudo Inverse
    # YOUR CODE HERE
   
    shape_=X.shape                                                              # To find out the shape of input matrix
    coloumn_matrix_ones = np.ones((shape_[0],1))                                # Creating a coloumn matrix of '1's as per size of input matrix
    X=np.append(X, coloumn_matrix_ones, axis=1)                                 # Appending coloumn of '1's as last coloumn of X.
    return(np.dot(np.dot(np.linalg.inv(np.add(np.dot(X.T,X),(np.identity(shape_[1]+1)*lamda))),X.T),t))   # Computing pseudo inverse as per the formula given in notebook.
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - other data'''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
np.testing.assert_array_almost_equal(Pseudo_Inverse(X,t,0.5),np.array([ 0.491,  0.183,  0.319, -0.002]),decimal=3)

"""#... Part 1 ends Below this you be more creative. Just comment out the lines where you save files (e.g. test predictions).

#**Part 2 begins ...**

**Instructions to be loosely followed (except number 8):**

1. Add more code and text cells between this and the last cell.
2. Read training data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv only. Do not use a local copy of the dataset.
3. Find the best lamda for **MSE+lamda*L2(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
4. Find the best lamda for **MSE+lamda*L1(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
5. Find the best lamda for the **pseudo-inv method**. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
6. Write your observations and conclusions.
7. Read test data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv only. Do not use a local copy of the dataset. Predict its dependent (missing last column) using the model with the lowest MSE, RMSE, or NRMSE. Save it as a file RollNo1_RollNo2_1.csv.
8. **Disable the prediction csv file saving statement and submit this entire .ipynb file, .py file, and .csv file as a single RollNo1_RollNo2_1.zip file.**

#**... Part 2 ends.**

1. Write the name or roll no.s of friends from outside your group with whom you discussed the assignment here (no penalty for mere discussion without copying code): 
2. Write the links of sources on the internet referred here (no penalty for mere consultation without copying code):
"""

import pandas as pd
url_train ="https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv" 
df_train =pd.read_csv(url_train)                                                                            #Reading CSV file from online repository to pandas dataframe.

url_test ="https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv"
df_test =pd.read_csv(url_test)

print(df_train.shape)                                                                                      # Verifying shape of pandas dataframe.   
print(df_test.shape)

from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()                                                               # Normalization with Min Max Scaler
min_max_scaler1 = preprocessing.MinMaxScaler()
min_max_scaler2 = preprocessing.MinMaxScaler()


t=df_train[['Next_Tmax']].to_numpy()                                                                        # Extracting the target vector (Next day temperature) from the Input vector
print(t.shape)
X=df_train.to_numpy()                                                                                       # converting pandas dataframe to numpy
print(X.shape)
X = np.delete(X, X.shape[1]-1, 1)                                                                           # Deleting the last coloumn(target vector) from numpy array.
print(X.shape)
#X=Normalize(X)                                                                                             # Min-Max normalization gives a better performance over mean-std deviation normalization.
#t=Normalize(t)                                                                                             # Therefore disabling mean-std deviation normalization.  

X = min_max_scaler.fit_transform(X)                                                                         # Min-max normalization
t = min_max_scaler1.fit_transform(t)                                                                        # Min-max normalization

index =np.arange(0, t.shape[0], 1, dtype=int)                                             # Linear linear array in incrementing order as per size of input data.
np.random.shuffle(index)                                                                  # Random shuffle the vector                                                           
slice_val =int(t.shape[0]*0.8)                                                            # Slice in 80:20 ratio
t_train=t[index[:slice_val]]                                                              # First 80% of shuffled data goes as train data (target label)
t_val=t[index[slice_val:]]                                                                # Remaining 20% of shuffled data goes as validation data(target label)
X_train=X[index[:slice_val]]                                                              # First 80% of shuffled data goes as train data (feature)
X_val=X[index[slice_val:]]                                                                # Remaining 20% of shuffled data goes as train data (feature)
print(t_train.shape,t_val.shape)                                                          #Printing shape of target and feature vector.
print(X_train.shape,X_val.shape)

w_initial=np.random.rand((X_train.shape[1]+1),1)                                # Randomly initialization of weight matrix
#print(X_train.shape,w.shape)
lamda_vector=[]                                                                 #Creating empty numpy arrays
validation_loss_vector=[]
train_loss_vector=[]
mse_validation_vector=[]
mse_train_vector=[]
lamda=0.0333
for i in range (0,400):                                                                                           # for loop of size 400
  w =w_initial                                                                                                    # start from unique starting point for each loop
  lamda =lamda+(0.001916675)                                                                                      # increment value for lamda
  results =Gradient_Descent (X_train, X_val, t_train, t_val, w, lamda, 5000, 1e-10, 1e-3, L2_Loss,L2_Gradient)    # Compute gradient descent algorithm  
  lamda_vector =np.append(lamda_vector,(1/lamda))                                                                 # Store (1/lamda information to output vector)
  train_loss_vector=np.append(train_loss_vector,results[1])                                                       # store train loss to vector
  validation_loss_vector=np.append(validation_loss_vector,results[2])                                             # store validation loss to vector    
  mse_train_vector=np.append(mse_train_vector,MSE_Loss(X_train,t_train,results[0]))                               # store mse of train data to vector
  mse_validation_vector=np.append(mse_validation_vector,MSE_Loss(X_val,t_val,results[0]))                         # store mse of validation data to vector

import os

from google.colab import drive                                                                                                  # storing output result to google drive for future analysis
drive.mount('/content/gdrive') 
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/validation_loss_vector_test1', validation_loss_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/train_loss_vector_test1', train_loss_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_validation_vector_test1', mse_validation_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_train_vector_test1', mse_train_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/lamda_vector_test1', lamda_vector)

import matplotlib.pyplot as plt                                                                         # Plotting output vectors with matplotlib 

import os                                                                                               # importing files from saved location

from google.colab import drive
drive.mount('/content/gdrive') 
validation_loss_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/validation_loss_vector_test1.npy')
train_loss_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/train_loss_vector_test1.npy')
mse_validation_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_validation_vector_test1.npy')
mse_train_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_train_vector_test1.npy')
lamda_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/lamda_vector_test1.npy')


fig = plt.figure(figsize=(8,8))
plt.clf()
plt.plot(lamda_vector,train_loss_vector,label='train_loss(L2)')
plt.plot(lamda_vector,validation_loss_vector,label='val_loss(L2')
plt.plot(lamda_vector,np.sqrt(mse_train_vector),label='rmse_train')
plt.plot(lamda_vector,np.sqrt(mse_validation_vector),label='rmse_val')
plt.xlabel('Model Complexity((1/Lamda))')
plt.ylabel('(Loss/RMSE)')
plt.title('RMSE and loss for training and validation data ')
plt.legend()

"""#### The lowest value of RMSE is obtained at (1/lamda)=5. Therefore best value of lamda =0.2."""

#print(X_train.shape,w.shape)
lamda=0.2
w =w_initial #w_initial is used from previous code block
results =Gradient_Descent (X_train, X_val, t_train, t_val, w, lamda, 5000, 1e-10, 1e-3, L2_Loss,L2_Gradient) 
validation_rmse=np.sqrt(MSE_Loss(X_val,t_val,results[0]))
validation_nrmse=NRMSE_Metric(X_val,t_val,results[0],0.2)
print("Weights for best lamda(0.2):",results[0])
print("Validation RMSE for best lamda(0.2):",validation_rmse)
print("Validation NRMSE for best lamda(0.2):",validation_nrmse)

w_initial=np.random.rand((X_train.shape[1]+1),1)                                                                  # Initializing random weight vector
#print(X_train.shape,w.shape)
lamda_vector=[]                                                                                                   # Creating null vectors
validation_loss_vector=[]
train_loss_vector=[]
mse_validation_vector=[]
mse_train_vector=[]
lamda_=0.01
for i in range (0,200):                                                                                           # for loop of size 200
  w =w_initial                                                                                                    # For each loop, start from unique starting point
  lamda =lamda_+(0.0164*i)
  results =Gradient_Descent (X_train, X_val, t_train, t_val, w, lamda, 5000, 1e-10, 1e-2, L1_Loss,L1_Gradient)    # compute gradient descent
  lamda_vector =np.append(lamda_vector,(1/lamda))                                                                 # Appending the results to the vector
  train_loss_vector=np.append(train_loss_vector,results[1])
  validation_loss_vector=np.append(validation_loss_vector,results[2])
  mse_train_vector=np.append(mse_train_vector,MSE_Loss(X_train,t_train,results[0]))
  mse_validation_vector=np.append(mse_validation_vector,MSE_Loss(X_val,t_val,results[0]))

import matplotlib.pyplot as plt

import os                                                                                               # importing files from saved location

from google.colab import drive                                                                          # Loading the values from already saved files.
drive.mount('/content/gdrive') 
validation_loss_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/validation_loss_vector_test2.npy')
train_loss_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/train_loss_vector_test2.npy')
mse_validation_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_validation_vector_test2.npy')
mse_train_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_train_vector_test2.npy')
lamda_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/lamda_vector_test2.npy')


fig = plt.figure(figsize=(12,12))
plt.clf()
plt.plot(lamda_vector,train_loss_vector,label='train_loss(L1)')
plt.plot(lamda_vector,validation_loss_vector,label='val_loss(L1')
plt.plot(lamda_vector,np.sqrt(mse_train_vector),label='rmse_train')
plt.plot(lamda_vector,np.sqrt(mse_validation_vector),label='rmse_val')
plt.xlabel('Model Complexity((1/Lamda))')
plt.ylabel('(Loss/RMSE)')
plt.title('RMSE and loss for training and validation data ')
plt.legend()

"""#### The lowest value of RMSE is obtained at (1/lamda)=16.89189. Therefore best value of lamda =0.0592."""

import os                                                                                                                 # Saving all the result to google drive for future use

from google.colab import drive
drive.mount('/content/gdrive') 
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/validation_loss_vector_test2', validation_loss_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/train_loss_vector_test2', train_loss_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_validation_vector_test2', mse_validation_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_train_vector_test2', mse_train_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/lamda_vector_test2', lamda_vector)

import matplotlib.pyplot as plt                                                                               # Ploting a zoomed portion of graph for better visibility
fig = plt.figure(figsize=(12,12))
plt.clf()

plt.plot(lamda_vector[2:6],np.sqrt(mse_train_vector[2:6]),label='rmse_train')
plt.plot(lamda_vector[2:6],np.sqrt(mse_validation_vector[2:6]),label='rmse_val')
plt.xlabel('Model Complexity((1/Lamda))')
plt.ylabel('(Loss/RMSE)')
plt.title('RMSE and loss for training and validation data ')
plt.legend()

#print(X_train.shape,w.shape)
w =w_initial #w_initial is used from previous code block
results =Gradient_Descent (X_train, X_val, t_train, t_val, w, (1/16.89189189), 5000, 1e-10, 1e-2, L1_Loss,L1_Gradient) 
validation_rmse=np.sqrt(MSE_Loss(X_val,t_val,results[0]))
validation_nrmse=NRMSE_Metric(X_val,t_val,results[0],(1/16.89189189))
print("Weights for best lamda(0.05920):",results[0])
print("Validation RMSE for best lamda(0.059):",validation_rmse)
print("Validation NRMSE for best lamda(0.059):",validation_nrmse)

#print(X_train.shape,w.shape)
lamda_vector=[]                                                                                   #creating empty numpy arrays
validation_loss_vector=[]
train_loss_vector=[]
mse_validation_vector=[]
mse_train_vector=[]
lamda=0
for i in range (0,100):                                                                           # For loop of 100                                                                  
  lamda =lamda+(0.01)                                                                             # Incrementing lambda
  weight =Pseudo_Inverse (X_train, t_train, lamda)                                                # Compute Pseudo Inverse operation
  lamda_vector =np.append(lamda_vector,(1/lamda))                                                 # Appending result to vector for future display
  train_loss_vector=np.append(train_loss_vector,L2_Loss (X_train, t_train, weight, lamda))
  validation_loss_vector=np.append(validation_loss_vector,L2_Loss (X_val, t_val, weight, lamda))
  mse_train_vector=np.append(mse_train_vector,MSE_Loss(X_train,t_train,weight))
  mse_validation_vector=np.append(mse_validation_vector,MSE_Loss(X_val,t_val,weight))

import os                                                                                                                       # Saving the output to google drive for future use

from google.colab import drive
drive.mount('/content/gdrive') 
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/validation_loss_vector_test3', validation_loss_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/train_loss_vector_test3', train_loss_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_validation_vector_test3', mse_validation_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_train_vector_test3', mse_train_vector)
np.save('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/lamda_vector_test3', lamda_vector)

import matplotlib.pyplot as plt

import os                                                                                               # importing files from saved location

from google.colab import drive                                                                          # Loading the values from already saved files.
drive.mount('/content/gdrive') 
validation_loss_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/validation_loss_vector_test3.npy')
train_loss_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/train_loss_vector_test3.npy')
mse_validation_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_validation_vector_test3.npy')
mse_train_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/mse_train_vector_test3.npy')
lamda_vector=np.load('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/lamda_vector_test3.npy')

fig = plt.figure(figsize=(8,8))
plt.clf()
plt.plot(lamda_vector,train_loss_vector,label='train_loss(L2)')
plt.plot(lamda_vector,validation_loss_vector,label='val_loss(L2')
plt.plot(lamda_vector,np.sqrt(mse_train_vector),label='rmse_train')
plt.plot(lamda_vector,np.sqrt(mse_validation_vector),label='rmse_val')
plt.xlabel('Model Complexity((1/Lamda))')
plt.ylabel('(Loss/RMSE)')
plt.title('RMSE and loss for training and validation data ')
plt.legend()

"""#### The lowest value of RMSE is obtained for lamda =0. RMSE curve is almost a straight line with very marginal slope."""

#print(X_train.shape,w.shape)
weight =Pseudo_Inverse (X_train, t_train, 0)
validation_rmse=np.sqrt(MSE_Loss(X_val,t_val,weight))
validation_nrmse=NRMSE_Metric(X_val,t_val,weight,0)
print("Weights for best lamda(0):",weight)
print("Validation RMSE for best lamda(0):",validation_rmse)
print("Validation NRMSE for best lamda(0):",validation_nrmse)

"""## Comparison of Models and observations.

#### Gradient descent with L2 regulalizer
**Best Lamda** = 0.2,**Validation RMSE** =0.1203,**Validation NRMSE** =0.8506
#### Gradient descent with L1 regulalizer
**Best Lamda** = 0.05920,**Validation RMSE** =0.14108,**Validation NRMSE** :0.9847
#### Pseudo Inverse Method
**Best Lamda** = 0,**Validation RMSE** =0.06319,**Validation NRMSE** =0.4496

### **The lowest value of RMSE is observed for Pseudo Inverse method. Therefore Pseudo Inverse is the best model out of these 3.**

Other observations
1. For L2 regularization, RMSE loss shows a distinct convex behavior.
2. For L1 regularizatin, RMSE loss shows a very small dip in the curve registering the best value of lambda.
3. For Pseudo Inverse method, RMSE loss is almost a straight line with best lambda at '0'.
"""

X_test=df_test.to_numpy()                                                                               # Min-Max normalization of test data.
print(X_test.shape)
X_test = min_max_scaler2.fit_transform(X_test)

print(X_test.shape)
print(weight.shape)
predicted_value =Prediction(X_test,weight)                                                            # Predicting Next day temperature for test data.
print(predicted_value.shape)
t_test=min_max_scaler1.inverse_transform(predicted_value)                                             # Denormalizing the output
print(t_test)
df_test['Next_Tmax'] = t_test.tolist()                                                                # Adding the output to the exising pandas dataframe
#df_test.to_csv('/content/gdrive/MyDrive/Colab Notebooks/EE_769_Assignment1/213074001_213070006_1.csv')  # Saving the output to CSV file . CSV File saving disabled as per instruction no 8
#df_test.head()                                                                                          # Printing few rows of the result.

"""## R2 score for the best Model

The best model out of the all 3 model is pseudo Inverse model. 
"""

import matplotlib.pyplot as plt
fig = plt.figure(figsize=(8,8))
weight =Pseudo_Inverse (X_val, t_val, 0)                                                              # Finding weights for validation data
predicted_value =Prediction(X_val,weight)                                                             # Predicting the output

t_val=min_max_scaler1.inverse_transform(t_val)                                                        # Denormalizing the output  
predicted_value=min_max_scaler1.inverse_transform(predicted_value)                                    # Denormalizing the input feature vector

plt.scatter(t_val, predicted_value, c ="blue")                                                        #Plotting the scatter plot
plt.xlabel('Training samples for next day temperature')
plt.ylabel('Predicted samples for next day temperature')
plt.title('Scatter plot training vs predicted samples ')
plt.show()

R2_Score=1-np.divide(np.sum(np.square(np.subtract(t_val,predicted_value))),np.sum(np.square(np.subtract(t_val,np.mean(t_val))))) # Computing the R2 score
print("R2 score for predicted sample vs validation sample is:",R2_Score)